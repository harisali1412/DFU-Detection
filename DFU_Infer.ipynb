{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply EfficientNetB0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8608 images belonging to 4 classes.\n",
      "Found 1847 images belonging to 4 classes.\n",
      "Found 1845 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dir = 'Split_data/train'\n",
    "validation_dir = 'Split_data/val'\n",
    "test_dir = 'Split_data/test'\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "test_val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "validation_generator = test_val_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_val_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # Important for test set to evaluate model properly\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Definition and Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(4, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mubasher Manzoor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.7593 - loss: 0.6266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mubasher Manzoor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 2s/step - accuracy: 0.7594 - loss: 0.6261 - val_accuracy: 0.8424 - val_loss: 0.4329\n",
      "Epoch 2/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m490s\u001b[0m 2s/step - accuracy: 0.8479 - loss: 0.3781 - val_accuracy: 0.8506 - val_loss: 0.3894\n",
      "Epoch 3/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m487s\u001b[0m 2s/step - accuracy: 0.8665 - loss: 0.3518 - val_accuracy: 0.8592 - val_loss: 0.3731\n",
      "Epoch 4/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 2s/step - accuracy: 0.8680 - loss: 0.3398 - val_accuracy: 0.8565 - val_loss: 0.3577\n",
      "Epoch 5/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 2s/step - accuracy: 0.8763 - loss: 0.3207 - val_accuracy: 0.8619 - val_loss: 0.3418\n",
      "Epoch 6/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 2s/step - accuracy: 0.8776 - loss: 0.3046 - val_accuracy: 0.8614 - val_loss: 0.3579\n",
      "Epoch 7/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m482s\u001b[0m 2s/step - accuracy: 0.8853 - loss: 0.2971 - val_accuracy: 0.8674 - val_loss: 0.3400\n",
      "Epoch 8/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m573s\u001b[0m 2s/step - accuracy: 0.8800 - loss: 0.2969 - val_accuracy: 0.8706 - val_loss: 0.3378\n",
      "Epoch 9/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1682s\u001b[0m 6s/step - accuracy: 0.8914 - loss: 0.2808 - val_accuracy: 0.8760 - val_loss: 0.3213\n",
      "Epoch 10/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1689s\u001b[0m 6s/step - accuracy: 0.8954 - loss: 0.2733 - val_accuracy: 0.8776 - val_loss: 0.3440\n",
      "Epoch 11/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 2s/step - accuracy: 0.8958 - loss: 0.2678 - val_accuracy: 0.8841 - val_loss: 0.3267\n",
      "Epoch 12/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m653s\u001b[0m 2s/step - accuracy: 0.8967 - loss: 0.2659 - val_accuracy: 0.8847 - val_loss: 0.3048\n",
      "Epoch 13/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m590s\u001b[0m 2s/step - accuracy: 0.9060 - loss: 0.2557 - val_accuracy: 0.8852 - val_loss: 0.3098\n",
      "Epoch 14/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 2s/step - accuracy: 0.9001 - loss: 0.2579 - val_accuracy: 0.8836 - val_loss: 0.3023\n",
      "Epoch 15/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m472s\u001b[0m 2s/step - accuracy: 0.9064 - loss: 0.2503 - val_accuracy: 0.8836 - val_loss: 0.3049\n",
      "Epoch 16/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m475s\u001b[0m 2s/step - accuracy: 0.9126 - loss: 0.2392 - val_accuracy: 0.8825 - val_loss: 0.2946\n",
      "Epoch 17/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 2s/step - accuracy: 0.9055 - loss: 0.2465 - val_accuracy: 0.8809 - val_loss: 0.3183\n",
      "Epoch 18/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m463s\u001b[0m 2s/step - accuracy: 0.9156 - loss: 0.2410 - val_accuracy: 0.8847 - val_loss: 0.2951\n",
      "Epoch 19/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 2s/step - accuracy: 0.9111 - loss: 0.2359 - val_accuracy: 0.8722 - val_loss: 0.2921\n",
      "Epoch 20/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 2s/step - accuracy: 0.9100 - loss: 0.2338 - val_accuracy: 0.8841 - val_loss: 0.2879\n",
      "Epoch 21/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m461s\u001b[0m 2s/step - accuracy: 0.9112 - loss: 0.2305 - val_accuracy: 0.8885 - val_loss: 0.2951\n",
      "Epoch 22/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 2s/step - accuracy: 0.9114 - loss: 0.2311 - val_accuracy: 0.8928 - val_loss: 0.2982\n",
      "Epoch 23/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9177 - loss: 0.2174 - val_accuracy: 0.8955 - val_loss: 0.2803\n",
      "Epoch 24/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m464s\u001b[0m 2s/step - accuracy: 0.9092 - loss: 0.2321 - val_accuracy: 0.8944 - val_loss: 0.2871\n",
      "Epoch 25/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m462s\u001b[0m 2s/step - accuracy: 0.9080 - loss: 0.2361 - val_accuracy: 0.8950 - val_loss: 0.2915\n",
      "Epoch 26/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m459s\u001b[0m 2s/step - accuracy: 0.9112 - loss: 0.2272 - val_accuracy: 0.8901 - val_loss: 0.2853\n",
      "Epoch 27/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m460s\u001b[0m 2s/step - accuracy: 0.9179 - loss: 0.2169 - val_accuracy: 0.8955 - val_loss: 0.2832\n",
      "Epoch 28/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m486s\u001b[0m 2s/step - accuracy: 0.9196 - loss: 0.2138 - val_accuracy: 0.9015 - val_loss: 0.2736\n",
      "Epoch 29/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 2s/step - accuracy: 0.9246 - loss: 0.1994 - val_accuracy: 0.8863 - val_loss: 0.2816\n",
      "Epoch 30/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m597s\u001b[0m 2s/step - accuracy: 0.9207 - loss: 0.2120 - val_accuracy: 0.8939 - val_loss: 0.2635\n",
      "Epoch 31/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m651s\u001b[0m 2s/step - accuracy: 0.9220 - loss: 0.2031 - val_accuracy: 0.8923 - val_loss: 0.2710\n",
      "Epoch 32/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m481s\u001b[0m 2s/step - accuracy: 0.9181 - loss: 0.2059 - val_accuracy: 0.8928 - val_loss: 0.2750\n",
      "Epoch 33/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9229 - loss: 0.2006 - val_accuracy: 0.8944 - val_loss: 0.2630\n",
      "Epoch 34/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9179 - loss: 0.2099 - val_accuracy: 0.8933 - val_loss: 0.2759\n",
      "Epoch 35/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9213 - loss: 0.1998 - val_accuracy: 0.8998 - val_loss: 0.2814\n",
      "Epoch 36/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 2s/step - accuracy: 0.9244 - loss: 0.1971 - val_accuracy: 0.8971 - val_loss: 0.2730\n",
      "Epoch 37/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m469s\u001b[0m 2s/step - accuracy: 0.9205 - loss: 0.1957 - val_accuracy: 0.8982 - val_loss: 0.2742\n",
      "Epoch 38/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 2s/step - accuracy: 0.9263 - loss: 0.1917 - val_accuracy: 0.8901 - val_loss: 0.2822\n",
      "Epoch 39/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9268 - loss: 0.1876 - val_accuracy: 0.8944 - val_loss: 0.2708\n",
      "Epoch 40/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9251 - loss: 0.1973 - val_accuracy: 0.8977 - val_loss: 0.2696\n",
      "Epoch 41/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9228 - loss: 0.1892 - val_accuracy: 0.9047 - val_loss: 0.2817\n",
      "Epoch 42/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 2s/step - accuracy: 0.9276 - loss: 0.1854 - val_accuracy: 0.9015 - val_loss: 0.2875\n",
      "Epoch 43/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9299 - loss: 0.1823 - val_accuracy: 0.8955 - val_loss: 0.2861\n",
      "Epoch 44/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 2s/step - accuracy: 0.9313 - loss: 0.1805 - val_accuracy: 0.9058 - val_loss: 0.2654\n",
      "Epoch 45/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9279 - loss: 0.1810 - val_accuracy: 0.9004 - val_loss: 0.2818\n",
      "Epoch 46/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m467s\u001b[0m 2s/step - accuracy: 0.9280 - loss: 0.1871 - val_accuracy: 0.8950 - val_loss: 0.2722\n",
      "Epoch 47/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 2s/step - accuracy: 0.9272 - loss: 0.1823 - val_accuracy: 0.8950 - val_loss: 0.2692\n",
      "Epoch 48/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 2s/step - accuracy: 0.9283 - loss: 0.1793 - val_accuracy: 0.9009 - val_loss: 0.2818\n",
      "Epoch 49/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m465s\u001b[0m 2s/step - accuracy: 0.9283 - loss: 0.1823 - val_accuracy: 0.9020 - val_loss: 0.2828\n",
      "Epoch 50/50\n",
      "\u001b[1m269/269\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m466s\u001b[0m 2s/step - accuracy: 0.9277 - loss: 0.1748 - val_accuracy: 0.8982 - val_loss: 0.3065\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.8410 - loss: 0.4790\n",
      "Test Accuracy: 0.8834688067436218\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "    img_array = preprocess_input(img_array)  # Preprocess the image according to EfficientNet standards\n",
    "    return img_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the previously saved model\n",
    "model = load_model('infer_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict Image Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 190ms/step\n",
      "Predicted class: Ischaemia\n"
     ]
    }
   ],
   "source": [
    "def predict_image_class(img_path, model, class_indices, threshold=0.6):\n",
    "    \"\"\"\n",
    "    Predict the class of an image. If the model's confidence is below the threshold, classify as \"Unknown\".\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to the input image.\n",
    "        model (Model): Trained model to use for prediction.\n",
    "        class_indices (dict): Class indices from the training data.\n",
    "        threshold (float): Confidence threshold for determining if the prediction is \"Unknown\".\n",
    "    \n",
    "    Returns:\n",
    "        str: Predicted class label or \"Unknown\".\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add a batch dimension\n",
    "    img_array = preprocess_input(img_array)  # Preprocess the image according to EfficientNet standards\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)\n",
    "    predicted_class_index = np.argmax(predictions, axis=1)  # Index of the highest probability\n",
    "    confidence = np.max(predictions)  # Highest probability value\n",
    "\n",
    "    # Get the class labels (only actual class names, not folder names)\n",
    "    labels = {0: \"Abnormal\", 1: \"Infection\", 2: \"Ischaemia\", 3: \"Normal\"}\n",
    "\n",
    "    # Check confidence threshold\n",
    "    if confidence < threshold:\n",
    "        return \"Unknown\"  # Confidence is below the threshold\n",
    "    else:\n",
    "        return labels[predicted_class_index[0]]  # Return the predicted class label\n",
    "\n",
    "\n",
    "# Assuming that you have `train_generator.class_indices` and it correctly maps to the actual classes\n",
    "class_indices = train_generator.class_indices\n",
    "\n",
    "# Test image\n",
    "img_path = 'Split_data/test/resized_images_of_Ischaemia/resized_image_22.jpg'  # Provide a path to an image not in your training classes\n",
    "predicted_class = predict_image_class(img_path, model, class_indices, threshold=0.7)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model in .keras format\n",
    "model.save(\"infer_model.keras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mubasher Manzoor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\saving\\saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 10 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_model = load_model(\"infer_model.keras\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
